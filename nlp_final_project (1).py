# -*- coding: utf-8 -*-
"""NLP_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LEx9z3lty1q0E0yWHDuehuOzrnSJLZRR
"""

from google.colab import files
import pandas as pd

uploaded = files.upload()
df = pd.read_csv("sentimentdataset.csv")
df.head()

df.info()
print(df.isnull().sum())
print(df.columns)
df.head()

import matplotlib.pyplot as plt
sentiment_counts = df['Sentiment'].value_counts()

plt.figure(figsize=(8, 5))
sentiment_counts.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])
plt.title("Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

sentiment_proportions = sentiment_counts / len(df) * 100
print("Sentiment Proportions (%):\n", sentiment_proportions)

platform_sentiment = df.groupby(['Platform', 'Sentiment']).size().unstack()

platform_sentiment.plot(kind='bar', figsize=(10, 6), stacked=True, colormap='viridis')
plt.title("Sentiment Distribution Across Platforms")
plt.xlabel("Platform")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.legend(title="Sentiment")
plt.show()

print("Platform-Based Sentiment Counts:\n", platform_sentiment)

import seaborn as sns

plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='Sentiment', y='Retweets', palette='Set2')
plt.title("Retweets by Sentiment")
plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='Sentiment', y='Likes', palette='Set3')
plt.title("Likes by Sentiment")
plt.show()

engagement_summary = df.groupby('Sentiment')[['Retweets', 'Likes']].mean()
print("Average Retweets and Likes by Sentiment:\n", engagement_summary)

import matplotlib.pyplot as plt

engagement_summary = df.groupby('Sentiment')[['Retweets', 'Likes']].mean()

plt.figure(figsize=(12, 6))
engagement_summary['Retweets'].sort_values(ascending=False).head(20).plot(kind='bar', color='skyblue')
plt.title("Top 20 Sentiments by Average Retweets")
plt.xlabel("Sentiment")
plt.ylabel("Average Retweets")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 6))
engagement_summary['Likes'].sort_values(ascending=False).head(20).plot(kind='bar', color='lightgreen')
plt.title("Top 20 Sentiments by Average Likes")
plt.xlabel("Sentiment")
plt.ylabel("Average Likes")
plt.xticks(rotation=45)
plt.show()

sentiment_counts = df['Sentiment'].value_counts()
print(sentiment_counts)

sentiment_mapping = {
    'Positive': 'Positive',
    'Joy': 'Positive',
    'Excitement': 'Positive',
    'Contentment': 'Positive',
    'Admiration': 'Positive',
    'Motivation': 'Positive',
    'Harmony': 'Positive',
    'Love': 'Positive',
    'Radiance': 'Positive',
    'Sadness': 'Negative',
    'Disappointment': 'Negative',
    'Fear': 'Negative',
    'Frustration': 'Negative',
    'Anger': 'Negative',
    'Neutral': 'Neutral',
    'Indifference': 'Neutral',
}

default_category = 'Neutral'

unmapped_sentiments = df[~df['Sentiment'].isin(sentiment_mapping.keys())]['Sentiment'].unique()
print("Unmapped Sentiments:\n", unmapped_sentiments)

sentiment_mapping = {
    'Positive': 'Positive',
    'Joy': 'Positive',
    'Excitement': 'Positive',
    'Contentment': 'Positive',
    'Happiness': 'Positive',
    'Admiration': 'Positive',
    'Motivation': 'Positive',
    'Love': 'Positive',
    'Amusement': 'Positive',
    'Gratitude': 'Positive',
    'Anticipation': 'Positive',
    'Euphoria': 'Positive',
    'Wonder': 'Positive',
    'Pride': 'Positive',
    'Harmony': 'Positive',
    'Thrill': 'Positive',
    'Enthusiasm': 'Positive',
    'Inspiration': 'Positive',
    'Satisfaction': 'Positive',
    'Negative': 'Negative',
    'Anger': 'Negative',
    'Fear': 'Negative',
    'Sadness': 'Negative',
    'Disgust': 'Negative',
    'Frustration': 'Negative',
    'Disappointment': 'Negative',
    'Despair': 'Negative',
    'Grief': 'Negative',
    'Heartbreak': 'Negative',
    'Loneliness': 'Negative',
    'Jealousy': 'Negative',
    'Anxiety': 'Negative',
    'Resentment': 'Negative',
    'Boredom': 'Negative',
    'Helplessness': 'Negative',
    'Regret': 'Negative',
    'Betrayal': 'Negative',
    'Sorrow': 'Negative',
    'Neutral': 'Neutral',
    'Indifference': 'Neutral',
    'Calmness': 'Neutral',
    'Serenity': 'Neutral',
    'Ambivalence': 'Neutral',
    'Curiosity': 'Neutral',
    'Acceptance': 'Neutral',
}

default_category = 'Neutral'

df['Refined_Sentiment'] = df['Sentiment'].map(sentiment_mapping).fillna(default_category)

print("Refined Sentiment Distribution:\n", df['Refined_Sentiment'].value_counts())

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
df['Refined_Sentiment'].value_counts().plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])
plt.title("Refined Sentiment Distribution")
plt.xlabel("Refined Sentiment")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

unmapped_sentiments = df[~df['Sentiment'].isin(sentiment_mapping.keys())]['Sentiment'].unique()
print("Unmapped Sentiments:\n", unmapped_sentiments)

def map_sentiment(sentiment):
    sentiment = sentiment.strip().lower()
    if any(word in sentiment for word in [
        'positive', 'joy', 'happiness', 'love', 'gratitude', 'excite',
        'admiration', 'pride', 'satisfaction', 'inspiration', 'wonder',
        'calmness', 'serenity', 'euphoria', 'thrill', 'motivation',
        'anticipation', 'amusement', 'playful', 'optimism'
    ]):
        return 'Positive'

    elif any(word in sentiment for word in [
        'negative', 'anger', 'fear', 'sad', 'frustration', 'anxiety',
        'disappointment', 'despair', 'grief', 'loneliness', 'regret',
        'betrayal', 'sorrow', 'melancholy', 'resentment', 'boredom',
        'jealousy', 'helplessness', 'disgust'
    ]):
        return 'Negative'

    elif any(word in sentiment for word in [
        'neutral', 'indifference', 'numbness', 'ambivalence', 'calmness'
    ]):
        return 'Neutral'

    return 'Neutral'

df['Refined_Sentiment'] = df['Sentiment'].apply(map_sentiment)

print("Refined Sentiment Distribution:\n", df['Refined_Sentiment'].value_counts())

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 5))
df['Refined_Sentiment'].value_counts().plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])
plt.title("Refined Sentiment Distribution")
plt.xlabel("Refined Sentiment")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

!pip install nltk

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import nltk
import shutil

shutil.rmtree('/root/nltk_data', ignore_errors=True)

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import re

def preprocess_text_simple(text):

    text = text.lower()

    text = re.sub(r"http\S+|www\S+|https\S+", '', text)

    text = re.sub(r'\@\w+|\#', '', text)

    text = re.sub(r"[^a-zA-Z\s]", '', text)

    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['Cleaned_Text'] = df['Text'].apply(preprocess_text_simple)

print(df[['Text', 'Cleaned_Text']].head())

from collections import Counter
import matplotlib.pyplot as plt

all_words = ' '.join(df['Cleaned_Text']).split()

word_freq = Counter(all_words)
common_words = word_freq.most_common(20)

common_words_df = pd.DataFrame(common_words, columns=['Word', 'Frequency'])

plt.figure(figsize=(10, 6))
plt.bar(common_words_df['Word'], common_words_df['Frequency'], color='skyblue')
plt.title('Top 20 Most Common Words')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

from wordcloud import WordCloud

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Cleaned Text')
plt.show()

df['Text_Length'] = df['Cleaned_Text'].apply(len)

plt.figure(figsize=(10, 6))
plt.hist(df['Text_Length'], bins=20, color='salmon', alpha=0.7)
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

avg_text_length = df.groupby('Refined_Sentiment')['Text_Length'].mean()

avg_text_length.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'], figsize=(8, 5))
plt.title('Average Text Length by Sentiment')
plt.xlabel('Refined Sentiment')
plt.ylabel('Average Text Length')
plt.xticks(rotation=0)
plt.show()

for sentiment in df['Refined_Sentiment'].unique():
    sentiment_words = ' '.join(df[df['Refined_Sentiment'] == sentiment]['Cleaned_Text']).split()
    sentiment_word_freq = Counter(sentiment_words).most_common(10)
    print(f"Top words for {sentiment} sentiment:\n", sentiment_word_freq, "\n")

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]
    return ' '.join(filtered_words)

df['Cleaned_Text_NoStopwords'] = df['Cleaned_Text'].apply(remove_stopwords)

print(df[['Cleaned_Text', 'Cleaned_Text_NoStopwords']].head())

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')

tfidf_matrix = tfidf_vectorizer.fit_transform(df['Cleaned_Text_NoStopwords'])

tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

print(tfidf_df.head())

tfidf_df['Refined_Sentiment'] = df['Refined_Sentiment']
tfidf_df.to_csv("tfidf_features.csv", index=False)

print("TF-IDF features saved as tfidf_features.csv")

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=100, random_state=42)
tfidf_reduced = svd.fit_transform(tfidf_matrix)

reduced_df = pd.DataFrame(tfidf_reduced, columns=[f"Component_{i}" for i in range(1, 101)])

reduced_df['Refined_Sentiment'] = df['Refined_Sentiment']

reduced_df.to_csv("reduced_tfidf_features.csv", index=False)
print("Reduced TF-IDF features saved as reduced_tfidf_features.csv")

df['Text_Length'] = df['Cleaned_Text_NoStopwords'].apply(len)

df['Word_Count'] = df['Cleaned_Text_NoStopwords'].apply(lambda x: len(x.split()))

reduced_df['Text_Length'] = df['Text_Length']
reduced_df['Word_Count'] = df['Word_Count']

reduced_df.to_csv("enhanced_tfidf_features.csv", index=False)
print("Enhanced features saved as enhanced_tfidf_features.csv")

from textblob import TextBlob

df['Polarity'] = df['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['Subjectivity'] = df['Text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
df['Text_Length'] = df['Text'].apply(len)
df['Word_Count'] = df['Text'].apply(lambda x: len(x.split()))

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=500)
tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text'])

from sklearn.decomposition import PCA

pca = PCA(n_components=100, random_state=42)
tfidf_reduced = pca.fit_transform(tfidf_matrix.toarray())

import numpy as np

numerical_features = df[['Polarity', 'Subjectivity', 'Text_Length', 'Word_Count']].values
combined_features = np.hstack([numerical_features, tfidf_reduced])

print("Combined Feature Shape:", combined_features.shape)

print(df.columns)

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

stop_words = list(ENGLISH_STOP_WORDS)

import re
import string

def preprocess_text(text):

    text = text.lower()

    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    text = text.translate(str.maketrans('', '', string.punctuation))

    text = re.sub(r'\d+', '', text)

    tokens = text.split()
    filtered_tokens = [word for word in tokens if word not in stop_words]

    return " ".join(filtered_tokens)

df['Cleaned_Text'] = df['Text'].apply(preprocess_text)

print(df[['Text', 'Cleaned_Text']].head())

def preprocess_text(text):

    text = text.lower()

    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    text = re.sub(r'[^\w\s]', '', text)

    text = re.sub(r'\d+', '', text)

    tokens = text.split()
    filtered_tokens = [word for word in tokens if word not in stop_words]

    return " ".join(filtered_tokens)

from collections import Counter
import matplotlib.pyplot as plt

all_text = " ".join(df['Cleaned_Text'])

words = all_text.split()

word_counts = Counter(words)
most_common_words = word_counts.most_common(10)

common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])

plt.figure(figsize=(10, 6))
plt.bar(common_words_df['Word'], common_words_df['Frequency'])
plt.title("Top 10 Most Frequent Words")
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.xticks(rotation=45)
plt.show()

common_words_df

from gensim.models import Word2Vec
import numpy as np

tokenized_sentences = df['Cleaned_Text'].apply(lambda x: x.split())

word2vec_model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=100,
    window=5,
    min_count=1,
    workers=4
)

def get_avg_word2vec(tokens, model, vector_size):
    """Generate average Word2Vec embedding for a document."""
    if len(tokens) < 1:
        return np.zeros(vector_size)
    vectors = [model.wv[word] for word in tokens if word in model.wv]
    if len(vectors) == 0:
        return np.zeros(vector_size)
    return np.mean(vectors, axis=0)

df['Word2Vec_Embedding'] = tokenized_sentences.apply(
    lambda tokens: get_avg_word2vec(tokens, word2vec_model, 100)
)

print("Sample Word2Vec Embedding:")
print(df['Word2Vec_Embedding'].head())

from sklearn.model_selection import train_test_split
import numpy as np

X_word2vec = np.stack(df['Word2Vec_Embedding'].values)
y_labels = df['Refined_Sentiment']

X_train_word2vec, X_test_word2vec, y_train, y_test = train_test_split(
    X_word2vec, y_labels, test_size=0.2, random_state=42, stratify=y_labels
)

X_train_word2vec.shape, X_test_word2vec.shape, len(y_train), len(y_test)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_word2vec, y_train)

y_test_pred = model.predict(X_test_word2vec)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Test Accuracy: {test_accuracy:.4f}")

print("Classification Report:\n", classification_report(y_test, y_test_pred))

from imblearn.over_sampling import RandomOverSampler
from collections import Counter

oversampler = RandomOverSampler(random_state=42)
X_train_balanced, y_train_balanced = oversampler.fit_resample(X_train_word2vec, y_train)

print("Class distribution after random oversampling:", Counter(y_train_balanced))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

logistic_model = LogisticRegression(random_state=42, max_iter=1000)
logistic_model.fit(X_train_balanced, y_train_balanced)

y_pred = logistic_model.predict(X_test_word2vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # Adjust max_features as needed
X_tfidf = tfidf_vectorizer.fit_transform(df['Cleaned_Text']).toarray()

import numpy as np

additional_features = df[['Polarity', 'Subjectivity', 'Text_Length', 'Word_Count']].values
X_combined = np.hstack((X_tfidf, additional_features))

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_combined, df['Refined_Sentiment'])

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

model = LogisticRegression(max_iter=500)

y_combined = y_resampled

print(X_combined.shape, len(y_combined))

y_original = df['Refined_Sentiment']
print(y_original.shape)

y_original = y_original.iloc[:X_combined.shape[0]]

y_combined = y_original

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000, random_state=42)

scores = cross_val_score(model, X_combined, y_combined, cv=5, scoring='accuracy')
print("Cross-Validation Accuracy (increased iterations): ", scores.mean())

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.utils import shuffle
from sklearn.exceptions import ConvergenceWarning
import warnings

warnings.filterwarnings("ignore", category=ConvergenceWarning)

X_combined, y_combined = shuffle(X_combined, y_combined, random_state=42)

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42))
])

cv_scores = cross_val_score(pipeline, X_combined, y_combined, cv=5, scoring='accuracy')

print("Cross-Validation Accuracy (Logistic Regression):", cv_scores.mean())

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

model = LogisticRegression(max_iter=1000, random_state=42)

model.fit(X_train_scaled, y_train)

print("Model fitted successfully.")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

balanced_model = LogisticRegression(
    solver='lbfgs',
    max_iter=1000,
    class_weight='balanced',
    random_state=42
)

balanced_model.fit(X_train_scaled, y_train)

test_accuracy = balanced_model.score(X_test_scaled, y_test)
y_test_pred = balanced_model.predict(X_test_scaled)

print("Test Accuracy (with balanced class weights):", test_accuracy)
print("Classification Report (with balanced class weights):\n", classification_report(y_test, y_test_pred))

cm = confusion_matrix(y_test, y_test_pred, labels=balanced_model.classes_)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=balanced_model.classes_, yticklabels=balanced_model.classes_)
plt.title("Confusion Matrix (with balanced class weights)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import joblib
joblib.dump(balanced_model, 'balanced_logistic_regression_model.pkl')
print("Model saved successfully.")

from sklearn.metrics import roc_curve, roc_auc_score

y_test_probs = balanced_model.predict_proba(X_test_scaled)
for i, class_label in enumerate(balanced_model.classes_):
    fpr, tpr, _ = roc_curve((y_test == class_label).astype(int), y_test_probs[:, i])
    auc_score = roc_auc_score((y_test == class_label).astype(int), y_test_probs[:, i])
    plt.plot(fpr, tpr, label=f"Class {class_label} (AUC = {auc_score:.2f})")

plt.plot([0, 1], [0, 1], 'k--', label="Random")
plt.title("One-vs-All ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

import joblib

loaded_model = joblib.load('balanced_logistic_regression_model.pkl')
print("Model loaded successfully.")

from google.colab import files

uploaded = files.upload()

import pandas as pd
dataset = pd.read_csv("sentimentdataset.csv")

!pip install transformers datasets torch scikit-learn

def simplify_sentiment(sentiment):
    sentiment = sentiment.strip().lower()
    if sentiment in ["positive", "happiness", "joy", "excitement", "gratitude", "love", "admiration"]:
        return 0  # Positive
    elif sentiment in ["negative", "anger", "fear", "sadness", "frustration", "disgust", "regret"]:
        return 1  # Negative
    elif sentiment in ["neutral", "calmness", "indifference"]:
        return 2  # Neutral
    else:
        return None

dataset['Sentiment'] = dataset['Sentiment'].apply(simplify_sentiment)
dataset = dataset.dropna(subset=['Sentiment'])

from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(dataset[['Text', 'Sentiment']], test_size=0.2, random_state=42)

from transformers import AutoTokenizer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_data(batch):
    return tokenizer(batch['Text'], padding=True, truncation=True, max_length=256)

train_data = train_data.reset_index(drop=True)
test_data = test_data.reset_index(drop=True)

train_data_tokenized = tokenizer(list(train_data['Text']), truncation=True, padding=True, max_length=256)
test_data_tokenized = tokenizer(list(test_data['Text']), truncation=True, padding=True, max_length=256)

import torch
from torch.utils.data import DataLoader, Dataset

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = SentimentDataset(train_data_tokenized, list(train_data['Sentiment']))
test_dataset = SentimentDataset(test_data_tokenized, list(test_data['Sentiment']))

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

print(train_dataset[0]['labels'])

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

print(train_dataset[0]['labels'])
print(type(train_dataset[0]['labels']))

train_labels = [int(label) for label in train_data['Sentiment']]
test_labels = [int(label) for label in test_data['Sentiment']]

train_dataset = SentimentDataset(train_data_tokenized, train_labels)
test_dataset = SentimentDataset(test_data_tokenized, test_labels)

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

train_labels = train_data['Sentiment'].astype(int).tolist()
test_labels = test_data['Sentiment'].astype(int).tolist()

train_dataset = SentimentDataset(train_data_tokenized, train_labels)
test_dataset = SentimentDataset(test_data_tokenized, test_labels)

print(train_dataset[0]['labels'])
print(type(train_dataset[0]['labels']))

from transformers import AutoModelForSequenceClassification

model_name = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

batch = next(iter(DataLoader(train_dataset, batch_size=16)))
outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])

print("Logits shape:", outputs.logits.shape)
print("Loss:", outputs.loss)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

metrics = trainer.evaluate()
print(metrics)

from transformers import pipeline

sentiment_analysis = pipeline("text-classification", model=model, tokenizer=tokenizer)

examples = [
    "This is the best day of my life!",
    "I am very disappointed with this product.",
    "It's an okay experience, not too bad, not too great."
]

results = sentiment_analysis(examples)
for example, result in zip(examples, results):
    print(f"Text: {example}\nPrediction: {result['label']} (Score: {result['score']:.4f})\n")

model.save_pretrained("./saved_model")
tokenizer.save_pretrained("./saved_model")

from transformers import AutoModelForSequenceClassification, AutoTokenizer

loaded_model = AutoModelForSequenceClassification.from_pretrained("./saved_model")
loaded_tokenizer = AutoTokenizer.from_pretrained("./saved_model")

print(model.config.id2label)

metrics = trainer.evaluate()
print(metrics)

label_mapping = {0: "Positive", 1: "Negative", 2: "Neutral"}

examples = [
    "This is the best day of my life!",
    "I am very disappointed with this product.",
    "It's an okay experience, not too bad, not too great."
]

results = sentiment_analysis(examples)

for example, result in zip(examples, results):
    sentiment = label_mapping[int(result['label'].split('_')[1])]
    print(f"Text: {example}\nPrediction: {sentiment} (Score: {result['score']:.4f})\n")

from sklearn.metrics import classification_report
import numpy as np

preds = trainer.predict(test_dataset)
y_pred = np.argmax(preds.predictions, axis=1)
y_true = test_data['Sentiment'].tolist()

print(classification_report(y_true, y_pred, target_names=["Positive", "Negative", "Neutral"]))

class_weights = torch.tensor(class_weights, dtype=torch.float).to("cpu")  # or simply omit .to("cpu")
print("Class weights:", class_weights)

from transformers import DistilBertForSequenceClassification
import torch.nn as nn

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

class CustomLossModel(nn.Module):
    def __init__(self, model, class_weights):
        super(CustomLossModel, self).__init__()
        self.model = model
        self.class_weights = class_weights
        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)

    def forward(self, input_ids, attention_mask, labels):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        loss = self.loss_fn(logits, labels)
        return {"loss": loss, "logits": logits}

custom_model = CustomLossModel(model, class_weights)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=1e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    report_to="none",
)

trainer = Trainer(
    model=custom_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

from sklearn.metrics import classification_report
import numpy as np

preds = trainer.predict(test_dataset)
y_pred = np.argmax(preds.predictions, axis=1)
y_true = test_data['Sentiment'].tolist()

print(classification_report(y_true, y_pred, target_names=["Positive", "Negative", "Neutral"]))

examples = [
    "I absolutely love this product!",
    "The experience was terrible and disappointing.",
    "It's just okay, nothing special.",
    "Amazing service! Highly recommended.",
    "The quality of this item is very poor."
]

results = sentiment_analysis(examples)
for example, result in zip(examples, results):
    print(f"Text: {example}\nPrediction: {result['label']} (Score: {result['score']:.4f})\n")

examples = [
    "I had the worst experience ever.",
    "This product is amazing and works perfectly!",
    "It's okay, not great but not bad either.",
    "Highly disappointed with the quality.",
    "Very neutral feeling about this decision."
]
results = sentiment_analysis(examples)
for example, result in zip(examples, results):
    print(f"Text: {example}\nPrediction: {result['label']} (Score: {result['score']:.4f})\n")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Positive", "Negative", "Neutral"])
disp.plot(cmap="Blues")

model.save_pretrained("./sentiment_model")
tokenizer.save_pretrained("./sentiment_model")

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("./sentiment_model")
tokenizer = AutoTokenizer.from_pretrained("./sentiment_model")

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

X = train_data['Text']
y = train_data['Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

max_words = 5000
max_len = 100

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

from tensorflow.keras.utils import to_categorical

y_train_onehot = to_categorical(y_train, num_classes=3)
y_test_onehot = to_categorical(y_test, num_classes=3)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    LSTM(128, return_sequences=False),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

history = model.fit(
    X_train_pad, y_train_onehot,
    validation_data=(X_test_pad, y_test_onehot),
    epochs=15,
    batch_size=32
)

loss, accuracy = model.evaluate(X_test_pad, y_test_onehot)
print(f"LSTM Test Accuracy: {accuracy:.2f}")

y_pred_probs = model.predict(X_test_pad)
y_pred = y_pred_probs.argmax(axis=1)

from sklearn.metrics import classification_report, confusion_matrix


y_test_labels = y_test.to_numpy()
print(classification_report(y_test_labels, y_pred, target_names=["Positive", "Negative", "Neutral"]))

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

model = AutoModelForSequenceClassification.from_pretrained("./sentiment_model")
tokenizer = AutoTokenizer.from_pretrained("./sentiment_model")

def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    predicted_label = torch.argmax(predictions, dim=1).item()
    labels = {0: "Positive", 1: "Negative", 2: "Neutral"}
    return labels[predicted_label], predictions[0][predicted_label].item()

text = "This is a great product!"
label, confidence = predict_sentiment(text)
print(f"Text: {text}\nLabel: {label}, Confidence: {confidence:.2f}")

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Transformer Model Accuracy: {accuracy * 100:.2f}%")